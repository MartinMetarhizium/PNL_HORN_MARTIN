#  Proyectos NLP

Este repositorio contiene una serie de notebooks desarrollados por **MartÃ­n Horn** como parte del proceso de aprendizaje en modelado de lenguaje y procesamiento de lenguaje natural (PLN), utilizando redes neuronales recurrentes (RNN) y LSTM. Se abordaron diferentes desafÃ­os, progresivamente mÃ¡s complejos, enfocados en la generaciÃ³n de texto a nivel de caracteres, usando datasets reales como letras de canciones, diÃ¡logos de los libros de Harry Potter y embeddings de fasttext.

---

## ğŸ“ Contenido

### ğŸ“˜ `Desafio_1_Horn_Martin.ipynb`
**Objetivo:** Entrenamiento de un modelo LSTM para generaciÃ³n de texto a nivel de palabras, usando letras de canciones.

- TokenizaciÃ³n por palabras
- Padding manual con `Keras`
- MÃ©trica de **perplejidad**
- Callback con early stopping
- PredicciÃ³n secuencial simple (`generate_seq`)

---

### ğŸ“˜ `Entregable_2_Horn_MartÃ­n.ipynb`
**Objetivo:** Reentrenamiento completo usando **tokenizaciÃ³n a nivel de caracteres**, tal como fue solicitado por el docente.

- Uso del dataset de letras de canciones (`scrapped-lyrics-from-6-genres`)
- TokenizaciÃ³n a nivel de carÃ¡cter
- Cambio del criterio de secuencias
- Mejora en la arquitectura LSTM
- Modelo mÃ¡s generalizable para generaciÃ³n letra por letra

---

### ğŸ“˜ `Entregable_3_Horn_Martin.ipynb`
**Objetivo:** Cambiar el corpus de entrenamiento a un dominio mÃ¡s cerrado y temÃ¡tico: **diÃ¡logos de Harry Potter**.

- Dataset de Kaggle: `Harry Potter Dataset` (libros 1, 2 y 3)
- Limpieza y uniÃ³n de archivos
- TokenizaciÃ³n por carÃ¡cter
- OptimizaciÃ³n del modelo:
  - Embedding de 128 dimensiones
  - LSTM con 256 unidades y dropout
  - `batch_size=128` y optimizador `Adam`
- VisualizaciÃ³n de:
  - Cross-entropy
  - Perplejidad por Ã©poca
- Gradio para generaciÃ³n interactiva
- ImplementaciÃ³n de **beam search** para mejorar la calidad del texto generado

---

### ğŸ¤– `Bot_Horn_Martin.ipynb`
**Objetivo:** Crear una interfaz interactiva con Gradio que permita generar texto a partir de una semilla, simulando una respuesta automÃ¡tica estilo chatbot.

- Entrenamiento del modelo en corpus reducido
- Uso de `Gradio` para desplegar un modelo generador interactivo
- AplicaciÃ³n del modelo en tareas prÃ¡cticas de NLP como:
  - autocompletado
  - generaciÃ³n creativa de texto
  - simulaciÃ³n de personaje

---

## ğŸš€ Principales aprendizajes

- TransiciÃ³n exitosa de modelado por palabras a modelado por caracteres
- Uso de mÃ©tricas de evaluaciÃ³n propias del lenguaje como la **perplejidad**
- Entrenamiento y depuraciÃ³n de modelos LSTM recurrentes
- OptimizaciÃ³n del rendimiento mediante arquitectura y datos
- Interactividad con interfaces modernas (`Gradio`)
- ExploraciÃ³n de estrategias de bÃºsqueda como **beam search**

---

## ğŸ“š TecnologÃ­as utilizadas

- Python 3.10
- TensorFlow / Keras
- Pandas / NumPy
- Matplotlib / Seaborn
- Gradio
- KaggleHub
- Jupyter Notebook

---

## ğŸ“Œ Requisitos

```bash
pip install tensorflow gradio pandas matplotlib kagglehub
```

---

## âœï¸ Autor

**MartÃ­n Horn**  
Estudiante de Inteligencia Artificial Embebida â€“ UBA  

